wide<-pivot_wider(data=vital,names_from = name,values_from = value,values_fill = NA) %>% rename(diastolic_blood_pressure="s_bp_noninvasive (d)",
systolic_blood_pressure="vs_bp_noninvasive (s)",
heart_rate="vs_hr_hr",respiratory_rate="xp_resp_rate_pt",SpO2="xp_resp_spo2")
skim(wide)
data.frame(var=c("diastolic_blood_pressure","systolic_blood_pressure","heart_rate","respiratory_rate","SpO2"),kurtosis=c(kurtosis(wide$diastolic_blood_pressure),kurtosis(wide$systolic_blood_pressure),kurtosis(wide$heart_rate) ,kurtosis(wide$respiratory_rate),kurtosis(wide$SpO2)),skewness=c(skewness(wide$diastolic_blood_pressure),skewness(wide$systolic_blood_pressure),skewness(wide$heart_rate) ,skewness(wide$respiratory_rate),skewness(wide$SpO2)))
alldata<-merge(wide,baseline,by.x="subject",by.y="mrn",all.y=TRUE)%>%
dplyr::select(-1)
set.seed(960515)
train_ind<-sample(nrow(alldata),nrow(alldata)/2)
train<-alldata[train_ind,]
test<-alldata[-train_ind,]
library(class)
KNN.error <- function(Train, Test, k=1) {
Train.x <- scale(Train[,which(names(Train) !="event")])
Test.x <- scale(Test[,which(names(Test) !="event")])
pred.class=knn(Train.x, Test.x, Train$event, k=k)
mean(Test$event != pred.class)
}
set.seed(960515)
train_num<-train
test_num<-test
indx <- sapply(train_num, is.factor)
train_num[indx] <- lapply(train_num[indx], function(x) as.numeric(x)-1)
indx <- sapply(test_num, is.factor)
test_num[indx] <- lapply(test_num[indx], function(x) as.numeric(x)-1)
#perform 5-fold cross-validation
error<-NULL
best_k=1
cv_error=1
folds <- cut(seq(1,nrow(train)),breaks=5,labels=FALSE)
for (k in 1:20){
for(m in 1:5){
testIndexes <- which(folds == m, arr.ind=TRUE)
cv_tr <- train_num[-testIndexes, ]
cv_tst <- train_num[testIndexes, ]
error[m]<-KNN.error(cv_tr,cv_tst,k)
}
if(cv_error>mean(error))
{
best_k<-k
cv_error<-mean(error)
}
}
KNN_error<-KNN.error(train_num,test_num,k=best_k)
logistic.error <- function(Train, Test) {
logistic.out <- glm(event ~., data=Train, family="binomial")
pred.class <- predict(logistic.out, Test, type="response")
pred.class <- ifelse(pred.class >0.5, "Yes", "No")
mean(Test$event != pred.class)
}
logistic_error<-logistic.error(train,test)
logistic_error
library(MASS)
LDA.error <- function(Train, Test) {
lda.out <- lda(event ~ ., data=Train)
pred.class = predict(lda.out, Test)$class
mean(Test$event != pred.class)
}
LDA_error<-LDA.error(train,test)
LDA_error
QDA.error <- function(Train, Test) {
qda.out <- qda(event ~ ., data=Train)
pred.class = predict(qda.out, Test)$class
mean(Test$event != pred.class)
}
QDA_error<-QDA.error(train,test)
QDA_error
library(bestglm)
backward.obj = glm(event ~ ., family="binomial", data=train)
bestForward<-stepAIC(backward.obj,direction="backward",trace=FALSE)
pred.class <- predict(bestForward, test, type="response")
pred.class <- ifelse(pred.class >0.50, "Yes", "No")
BackStp_error <- mean(test$event != pred.class)
BackStp_error
train_num[,which(names(alldata) !="event")]=scale(train_num[,which(names(alldata) !="event")])
test_num[,which(names(alldata) !="event")]=scale(test_num[,which(names(alldata) !="event")])
library(nnet)
set.seed(960515)
nnetGrid<-expand.grid(.size=25,.decay=c(0,0.001,0.01,0.1,1,5,10,20))
nnetfit<-caret::train(event~.,data=train_num,method="nnet",trControl=trainControl(method="cv",number=5),tuneGrid=nnetGrid,trace=F)
nnetfit$bestTune$decay
KNN_error<-KNN.error(train_num,test_num,k=best_k)
library(tidyverse)
library(skimr)
lab<-read.csv("lab and vitals.csv")
baseline<-read.csv("baselines.csv",stringsAsFactors=TRUE)
lab%>%
group_by(subject,name)%>%
summarise(ave=mean(value,na.rm=TRUE))%>%
rename(value=ave)%>%
ungroup()->vital
wide<-pivot_wider(data=vital,names_from = name,values_from = value,values_fill = NA)
skim(wide)
lab%>%
group_by(subject,name)%>%
summarise(ave=mean(value,na.rm=TRUE))%>%
rename(value=ave)%>%
ungroup()->vital
wide<-pivot_wider(data=vital,names_from = name,values_from = value,values_fill = NA)
alldata<-merge(wide,baseline,by.x="subject",by.y="mrn",all.y=TRUE)%>%
rename(s_bp_noninvasive="s_bp_noninvasive (d)",
vs_bp_noninvasive="vs_bp_noninvasive (s)")%>%
dplyr::select(-1)
set.seed(960515)
train_ind<-sample(nrow(alldata),nrow(alldata)/2)
train_x<-alldata[train_ind,which(names(alldata) !="event")]
train<-alldata[train_ind,]
train_y<-alldata[train_ind,"event"]
test_x<-alldata[-train_ind,which(names(alldata) !="event")]
test_y<-alldata[-train_ind,"event"]
test<-alldata[-train_ind,]
library(class)
KNN.error <- function(Train, Test, k=1) {
Train.x <- scale(Train[,which(names(Train) !="event")])
Test.x <- scale(Test[,which(names(Test) !="event")])
pred.class=knn(Train.x, Test.x, Train$event, k=k)
mean(Test$event != pred.class)
}
set.seed(960515)
train_num<-train
test_num<-test
indx <- sapply(train_num, is.factor)
train_num[indx] <- lapply(train_num[indx], function(x) as.numeric(x)-1)
indx <- sapply(test_num, is.factor)
test_num[indx] <- lapply(test_num[indx], function(x) as.numeric(x)-1)
#perform 5-fold cross-validation
error<-NULL
best_k=1
cv_error=1
folds <- cut(seq(1,nrow(train)),breaks=5,labels=FALSE)
for (k in 1:20){
for(m in 1:5){
testIndexes <- which(folds == m, arr.ind=TRUE)
cv_tr <- train_num[-testIndexes, ]
cv_tst <- train_num[testIndexes, ]
error[m]<-KNN.error(cv_tr,cv_tst,k)
}
if(cv_error>mean(error))
{
best_k<-k
cv_error<-mean(error)
}
}
best_k
KNN_error<-KNN.error(train_num,test_num,k=best_k)
KNN_error
logistic.error <- function(Train, Test) {
logistic.out <- glm(event ~., data=Train, family="binomial")
pred.class <- predict(logistic.out, Test, type="response")
pred.class <- ifelse(pred.class >0.5, "Yes", "No")
mean(Test$event != pred.class)
}
logistic_error<-logistic.error(train,test)
logistic_error
library(MASS)
LDA.error <- function(Train, Test) {
lda.out <- lda(event ~ ., data=Train)
pred.class = predict(lda.out, Test)$class
mean(Test$event != pred.class)
}
LDA_error<-LDA.error(train,test)
LDA_error
QDA.error <- function(Train, Test) {
qda.out <- qda(event ~ ., data=Train)
pred.class = predict(qda.out, Test)$class
mean(Test$event != pred.class)
}
QDA_error<-QDA.error(train,test)
QDA_error
library(bestglm)
backward.obj = glm(event ~ ., family="binomial", data=train)
bestForward<-stepAIC(backward.obj,direction="backward",trace=FALSE)
pred.class <- predict(bestForward, test, type="response")
pred.class <- ifelse(pred.class >0.50, "Yes", "No")
BackStp_error <- mean(test$event != pred.class)
BackStp_error
forward.obj = glm(event ~ 1, family="binomial", data=train)
bestForward<-stepAIC(forward.obj,direction="forward",scope = list(lower = formula(forward.obj), upper = formula(backward.obj)),trace=FALSE)
pred.class <- predict(bestForward, test, type="response")
pred.class <- ifelse(pred.class >0.50, "Yes", "No")
ForStp_error <- mean(test$event != pred.class)
ForStp_error
library(glmnet)
x=model.matrix(event~.,train)[,-1]
y=train$event
newx=model.matrix(event~.,test)[,-1]
newy=test$event
grid =10^seq(10,-2,length =100) ##define range of lambda values
##Run cross validation to choose lambda
set.seed(960515)
cv1=cv.glmnet(x,y, family="binomial",lambda=grid,nfolds=5,alpha=1)
cv1$lambda.1se
cv1.final = glmnet(x,y, family="binomial", lambda = cv1$lambda.1se,alpha=1)
pred.class <- predict(cv1.final, newx, type="response")
pred.class <- ifelse(pred.class >0.50,"Yes", "No")
Elast_alpha1_error <- mean(newy != pred.class)
Elast_alpha1_error
##Run cross validation to choose lambda
set.seed(960515)
cv.5=cv.glmnet(x,y, family="binomial",lambda=grid,nfolds=5,alpha=0.5)
cv.5$lambda.1se
cv.5.final = glmnet(x,y, family="binomial", lambda = cv.5$lambda.1se,alpha=.5)
pred.class <- predict(cv.5.final, newx, type="response")
pred.class <- ifelse(pred.class >0.50,"Yes", "No")
Elast_alpha.5_error <- mean(newy != pred.class)
Elast_alpha.5_error
##Run cross validation to choose lambda
set.seed(960515)
cv0=cv.glmnet(x,y, family="binomial",lambda=grid,nfolds=5,alpha=0)
cv0$lambda.1se
cv0.final = glmnet(x,y, family="binomial", lambda = cv0$lambda.1se,alpha=)
pred.class <- predict(cv0.final, newx, type="response")
pred.class <- ifelse(pred.class >0.50,"Yes", "No")
Elast_alpha0_error <- mean(newy != pred.class)
Elast_alpha0_error
library(tree)
set.seed(960515)
mytree<-tree(event~.,data=train,method = "gini")
mytree.cv<-cv.tree(mytree,FUN=prune.misclass,K=10)
mytree.cv$size[mytree.cv$dev==min(mytree.cv$dev)]
final.tree=prune.tree(mytree,best=mytree.cv$size[mytree.cv$dev==min(mytree.cv$dev)])
mypredict<-predict(final.tree,newdata=test,type="class")
tmp=table(mypredict,test$event)
tree_error<-1-sum(diag(tmp)/sum(tmp))
tree_error
library(randomForest)
set.seed(960515)
bagged.tree<-randomForest(event~., data = train, mtry = ncol(train)-1, ntree = 1000,importance=TRUE)
mypredict.bagg = predict(bagged.tree, test, type = "response")
tmp.bagg.tree <- table(mypredict.bagg, test$event)
bagg_error <- 1 - sum(diag(tmp.bagg.tree)/sum(tmp.bagg.tree))
bagg_error
set.seed(960515)
rf<-randomForest(event~., data = train, mtry = 6, ntree = 1000,importance=TRUE)
mypredict.rf = predict(rf, test, type = "response")
tmp.rf.tree <- table(mypredict.rf, test$event)
rf_error <- 1 - sum(diag(tmp.rf.tree)/sum(tmp.rf.tree))
rf_error
train_gbm<-train
test_gbm<-test
train_gbm$event.int<-as.integer(train_gbm$event)-1
test_gbm$event.int<-as.integer(test_gbm$event)-1
library(caret)
library(gbm)
set.seed(960515)
caretGrid.depth.1 <- expand.grid(interaction.depth = 1,
n.trees = 2000,
shrinkage = seq(0, 0.01, by = 0.001),
n.minobsinnode = 10)
gbm.d1 <- caret::train(event ~.-event.int , distribution = "bernoulli", data = train_gbm,
method = "gbm", trControl = trainControl(method="cv", number=5),
verbose = F, tuneGrid = caretGrid.depth.1)
gbm.d1$bestTune$shrinkage
gbm_d1 <- gbm(event.int ~ . -event , data = train_gbm, n.trees = 2000, interaction.depth = 1, shrinkage = gbm.d1$bestTune$shrinkage,  distribution = 'bernoulli')
gbm.d1_error=mean((predict(gbm_d1, newdata = test_gbm, n.trees = 2000, type = 'response') >0.5) != test_gbm$event.int)
gbm.d1_error
set.seed(960515)
caretGrid.depth.2 <- expand.grid(interaction.depth = 2,
n.trees = 2000,
shrinkage = seq(0, 0.01, by = 0.001),
n.minobsinnode = 10)
gbm.d2 <- caret::train(event ~.-event.int , distribution = "bernoulli", data = train_gbm,
method = "gbm", trControl = trainControl(method="cv", number=5),
verbose = F, tuneGrid = caretGrid.depth.2)
gbm.d2$bestTune$shrinkage
gbm_d2 <- gbm(event.int ~ . -event , data = train_gbm, n.trees = 2000, interaction.depth = 2, shrinkage = gbm.d2$bestTune$shrinkage,  distribution = 'bernoulli')
gbm_d2 <- gbm(event.int ~ . -event , data = train_gbm, n.trees = 2000, interaction.depth = 2, shrinkage = gbm.d2$bestTune$shrinkage,  distribution = 'bernoulli')
gbm.d2_error=mean((predict(gbm_d2, newdata = test_gbm, n.trees = 2000, type = 'response') >0.5) != test_gbm$event.int)
gbm.d2_error
set.seed(960515)
ada.d1 <- caret::train(event ~.-event.int , distribution = "adaboost", data = train_gbm,method = "gbm", trControl = trainControl(method="cv", number=5), verbose = F, tuneGrid = caretGrid.depth.1)
ada.d1$bestTune$shrinkage
ada_d1 <- gbm(event.int ~ . -event , data = train_gbm, n.trees = 2000, interaction.depth = 1, shrinkage = ada.d1$bestTune$shrinkage,  distribution = 'adaboost')
ada.d1_error=mean((predict(ada_d1, newdata = test_gbm, n.trees = 2000, type = 'response') >0.5) != test_gbm$event.int)
ada.d1_error
set.seed(960515)
ada.d2 <- caret::train(event ~.-event.int , distribution = "adaboost", data = train_gbm, method = "gbm", trControl = trainControl(method="cv", number=5),verbose = F, tuneGrid = caretGrid.depth.2)
ada.d2$bestTune$shrinkage
ada_d2 <- gbm(event.int ~ . -event , data = train_gbm, n.trees = 2000, interaction.depth = 2, shrinkage = ada.d2$bestTune$shrinkage,  distribution = 'adaboost')
ada.d2_error=mean((predict(ada_d2, newdata = test_gbm, n.trees = 2000, type = 'response') >0.5) != test_gbm$event.int)
ada.d2_error
library(e1071)
set.seed(960515)
tune<-tune.svm(event~.,data=train,kernel="linear",cost=c(0.001 , 0.01, 0.1, 1,5,10,100))
tune$best.parameters
svm.obj1<-svm(event~.-event,data=train,kernel="linear",cost=tune$best.parameters$cost)
svclass_error<-mean(predict(svm.obj1,newdata=test)!=test$event)
svclass_error
set.seed(960515)
tune<-tune.svm(event~.,data=train,kernel="radial",cost=c(0.001 , 0.01, 0.1, 1,5,10,100),gamma=c(0.1,0.01,0.001))
tune$best.parameters
svm.obj2<-svm(event~.-event,data=train,kernel="radial",cost=tune$best.parameters$cost,gamma=tune$best.parameters$gamma)
radial_error<-mean(predict(svm.obj2,newdata=test)!=test$event)
radial_error
train_num[,which(names(alldata) !="event")]=scale(train_num[,which(names(alldata) !="event")])
test_num[,which(names(alldata) !="event")]=scale(test_num[,which(names(alldata) !="event")])
train_num$event<-factor(train_num$event)
test_num$event<-factor(test_num$event)
library(nnet)
set.seed(960515)
nnetGrid<-expand.grid(.size=25,.decay=c(0,0.001,0.01,0.1,1,5,10,20))
nnetfit<-caret::train(event~.,data=train_num,method="nnet",trControl=trainControl(method="cv",number=5),tuneGrid=nnetGrid,trace=F)
nnetfit$bestTune$decay
neu.obj<-nnet(event~.,data=train_num,size=25,decay=10,trace=FALSE)
nn_error<-mean(ifelse(predict(neu.obj,newdata = test_num)>0.5,1,0)!=test_num$event)
nn_error
library(knitr)
library(kableExtra)
data.frame(Method=c("KNN","Logistic Regression","LDA","QDA", "Backward Stepwise Selection", "Forward Stepwise Selection", "LASSO", "Elastic Net (alpha=0.5)", "Ridege Regression", "A Sing Tree","Bagged Tree","Random Forest","A gradient boosted model (d=1)","A gradient boosted model (d=2)","Adaboost model (d=1)", "Adaboost model (d=2)","Support Vector classifier", "“radial” kernel","Neural Network" ), Test_Error=round(c(KNN_error, logistic_error, LDA_error, QDA_error, BackStp_error,ForStp_error,Elast_alpha1_error,Elast_alpha.5_error,Elast_alpha0_error,tree_error,bagg_error,rf_error,gbm.d1_error,gbm.d2_error,ada.d1_error,ada.d2_error,svclass_error,radial_error,nn_error), digits=3)) %>% kable() %>% kable_styling()
library(knitr)
library(kableExtra)
data.frame(Method=c("KNN","Logistic Regression","LDA","QDA", "Backward Stepwise Selection", "Forward Stepwise Selection", "LASSO", "Elastic Net (alpha=0.5)", "Ridege Regression", "A Sing Tree","Bagged Tree","Random Forest","Gradient Boosted Model (d=1)","Gradient Boosted Model (d=2)","Adaboost Model (d=1)", "Adaboost Model (d=2)","Support Vector Classifier", "Support Vector Machines with “Radial” Kernel","Neural Network" ), Test_Error=round(c(KNN_error, logistic_error, LDA_error, QDA_error, BackStp_error,ForStp_error,Elast_alpha1_error,Elast_alpha.5_error,Elast_alpha0_error,tree_error,bagg_error,rf_error,gbm.d1_error,gbm.d2_error,ada.d1_error,ada.d2_error,svclass_error,radial_error,nn_error), digits=3)) %>% kable() %>% kable_styling()
library(knitr)
library(kableExtra)
data.frame(Type=c("Classification Methods","","","","Stepwise Selection Methods","","Shrinkage Methods","","","Tree-Based Methods","","","","","","","Support Vector Machines","","Neural Network"),
Method=c("KNN","Logistic Regression","LDA","QDA", "Backward Stepwise Selection", "Forward Stepwise Selection", "LASSO", "Elastic Net (alpha=0.5)", "Ridege Regression", "A Sing Tree","Bagged Tree","Random Forest","Gradient Boosted Model (d=1)","Gradient Boosted Model (d=2)","Adaboost Model (d=1)", "Adaboost Model (d=2)","Support Vector Classifier", "Support Vector Machines with “Radial” Kernel","Neural Network" ), Test_Error=round(c(KNN_error, logistic_error, LDA_error, QDA_error, BackStp_error,ForStp_error,Elast_alpha1_error,Elast_alpha.5_error,Elast_alpha0_error,tree_error,bagg_error,rf_error,gbm.d1_error,gbm.d2_error,ada.d1_error,ada.d2_error,svclass_error,radial_error,nn_error), digits=3)) %>% kable() %>% kable_styling()
lda(event ~ ., data=Train)
lda(event ~ ., data=train)
lda.obj<-lda(event ~ ., data=train)
lda.obj<-lda(event ~ ., data=train)
lda.obj$scaling
lda.obj<-lda(event ~ ., data=train)
lda.obj$lev
lda.obj<-lda(event ~ ., data=train)
lda.obj$means
lda.obj<-lda(event ~ ., data=train)
lda.obj$scaling
plot(lda.obj)
plot(lda.obj$scaling)
lab%>%
group_by(subject,name)%>%
summarise(ave=mean(value,na.rm=TRUE))%>%
rename(value=ave)%>%
ungroup()->vital
wide<-pivot_wider(data=vital,names_from = name,values_from = value,values_fill = NA) %>% rename(diastolic_blood_pressure="s_bp_noninvasive (d)",
systolic_blood_pressure="vs_bp_noninvasive (s)",
heart_rate="vs_hr_hr",respiratory_rate="xp_resp_rate_pt",SpO2="xp_resp_spo2")
skim(wide)
alldata<-merge(wide,baseline,by.x="subject",by.y="mrn",all.y=TRUE)%>%
dplyr::select(-1)
set.seed(960515)
train_ind<-sample(nrow(alldata),nrow(alldata)/2)
train<-alldata[train_ind,]
test<-alldata[-train_ind,]
library(class)
KNN.error <- function(Train, Test, k=1) {
Train.x <- scale(Train[,which(names(Train) !="event")])
Test.x <- scale(Test[,which(names(Test) !="event")])
pred.class=knn(Train.x, Test.x, Train$event, k=k)
mean(Test$event != pred.class)
}
set.seed(960515)
train_num<-train
test_num<-test
indx <- sapply(train_num, is.factor)
train_num[indx] <- lapply(train_num[indx], function(x) as.numeric(x)-1)
indx <- sapply(test_num, is.factor)
test_num[indx] <- lapply(test_num[indx], function(x) as.numeric(x)-1)
#perform 5-fold cross-validation
error<-NULL
best_k=1
cv_error=1
folds <- cut(seq(1,nrow(train)),breaks=5,labels=FALSE)
for (k in 1:20){
for(m in 1:5){
testIndexes <- which(folds == m, arr.ind=TRUE)
cv_tr <- train_num[-testIndexes, ]
cv_tst <- train_num[testIndexes, ]
error[m]<-KNN.error(cv_tr,cv_tst,k)
}
if(cv_error>mean(error))
{
best_k<-k
cv_error<-mean(error)
}
}
KNN_error<-KNN.error(train_num,test_num,k=best_k)
logistic.error <- function(Train, Test) {
logistic.out <- glm(event ~., data=Train, family="binomial")
pred.class <- predict(logistic.out, Test, type="response")
pred.class <- ifelse(pred.class >0.5, "Yes", "No")
mean(Test$event != pred.class)
}
logistic_error<-logistic.error(train,test)
logistic_error
lda.obj<-lda(event ~ ., data=train)
lda.obj$scaling
data.frame(var=c("diastolic_blood_pressure","systolic_blood_pressure","heart_rate","respiratory_rate","SpO2"),kurtosis=c(kurtosis(wide$diastolic_blood_pressure),kurtosis(wide$systolic_blood_pressure),kurtosis(wide$heart_rate) ,kurtosis(wide$respiratory_rate),kurtosis(wide$SpO2)),skewness=c(skewness(wide$diastolic_blood_pressure),skewness(wide$systolic_blood_pressure),skewness(wide$heart_rate) ,skewness(wide$respiratory_rate),skewness(wide$SpO2)))%>% kable() %>% kable_styling()
data.frame(var=c("diastolic_blood_pressure","systolic_blood_pressure","heart_rate","respiratory_rate","SpO2"),kurtosis=c(kurtosis(wide$diastolic_blood_pressure),kurtosis(wide$systolic_blood_pressure),kurtosis(wide$heart_rate) ,kurtosis(wide$respiratory_rate),kurtosis(wide$SpO2)),skewness=c(skewness(wide$diastolic_blood_pressure),skewness(wide$systolic_blood_pressure),skewness(wide$heart_rate) ,skewness(wide$respiratory_rate),skewness(wide$SpO2)))%>% kable() %>% kable_styling()
lab<-read.csv("lab and vitals.csv")
baseline<-read.csv("baselines.csv",stringsAsFactors=TRUE)
library(tidyverse)
library(skimr)
skim(baseline)
lab%>%
group_by(subject,name)%>%
summarise(ave=mean(value,na.rm=TRUE))%>%
rename(value=ave)%>%
ungroup()->vital
wide<-pivot_wider(data=vital,names_from = name,values_from = value,values_fill = NA) %>% rename(diastolic_blood_pressure="s_bp_noninvasive (d)",
systolic_blood_pressure="vs_bp_noninvasive (s)",
heart_rate="vs_hr_hr",respiratory_rate="xp_resp_rate_pt",SpO2="xp_resp_spo2")
skim(wide)
data.frame(var=c("diastolic_blood_pressure","systolic_blood_pressure","heart_rate","respiratory_rate","SpO2"),kurtosis=c(kurtosis(wide$diastolic_blood_pressure),kurtosis(wide$systolic_blood_pressure),kurtosis(wide$heart_rate) ,kurtosis(wide$respiratory_rate),kurtosis(wide$SpO2)),skewness=c(skewness(wide$diastolic_blood_pressure),skewness(wide$systolic_blood_pressure),skewness(wide$heart_rate) ,skewness(wide$respiratory_rate),skewness(wide$SpO2)))%>% kable() %>% kable_styling()
lab%>%
group_by(subject,name)%>%
summarise(ave=mean(value,na.rm=TRUE))%>%
rename(value=ave)%>%
ungroup()->vital
wide<-pivot_wider(data=vital,names_from = name,values_from = value,values_fill = NA) %>% rename(diastolic_blood_pressure="s_bp_noninvasive (d)",
systolic_blood_pressure="vs_bp_noninvasive (s)",
heart_rate="vs_hr_hr",respiratory_rate="xp_resp_rate_pt",SpO2="xp_resp_spo2")
par(mfcol = c(1,5))
hist(wide$diastolic_blood_pressure, main = "Distribution of diastolic blood pressure", xlab = "diastolic_blood_pressure")
hist(wide$systolic_blood_pressure, main = "Distribution of systolic_blood_pressure", xlab = "systolic_blood_pressure")
hist(wide$heart_rate, main = "Distribution of heart_rate", xlab = "heart_rate")
hist(wide$respiratory_rate, main = "Distribution of respiratory_rate", xlab = "respiratory_rate")
hist(wide$SpO2, main = "Distribution of SpO2", xlab = "SpO2")
par(mfcol = c(1,5))
boxplot(wide$diastolic_blood_pressure, main = "Distribution of diastolic blood pressure", xlab = "diastolic_blood_pressure")
hist(wide$systolic_blood_pressure, main = "Distribution of systolic_blood_pressure", xlab = "systolic_blood_pressure")
hist(wide$heart_rate, main = "Distribution of heart_rate", xlab = "heart_rate")
hist(wide$respiratory_rate, main = "Distribution of respiratory_rate", xlab = "respiratory_rate")
hist(wide$SpO2, main = "Distribution of SpO2", xlab = "SpO2")
par(mfcol = c(2,3))
hist(wide$diastolic_blood_pressure, main = "Distribution of diastolic blood pressure", xlab = "diastolic_blood_pressure")
hist(wide$systolic_blood_pressure, main = "Distribution of systolic_blood_pressure", xlab = "systolic_blood_pressure")
hist(wide$heart_rate, main = "Distribution of heart_rate", xlab = "heart_rate")
hist(wide$respiratory_rate, main = "Distribution of respiratory_rate", xlab = "respiratory_rate")
hist(wide$SpO2, main = "Distribution of SpO2", xlab = "SpO2")
par(mfcol = c(1,3))
hist(baseline$age, main = "Distribution of age", xlab = "age")
par(mfcol = c(1,3))
hist(baseline$age, main = "Distribution of age", xlab = "age")
par(mfcol = c(1,3))
hist(baseline$Age, main = "Distribution of age", xlab = "age")
hist(baseline$bmi, main = "Distribution of bmi", xlab = "bmi")
hist(baseline$duration_sym, main = "Distribution of duration of symptoms", xlab = "duration of symptoms")
sum(is.na(baseline))/nrow(baseline)
par(mfcol = c(2,3))
hist(wide$diastolic_blood_pressure, main = "Distribution of diastolic blood pressure", xlab = "diastolic_blood_pressure")
hist(wide$systolic_blood_pressure, main = "Distribution of systolic_blood_pressure", xlab = "systolic_blood_pressure")
hist(wide$heart_rate, main = "Distribution of heart_rate", xlab = "heart_rate")
hist(wide$respiratory_rate, main = "Distribution of respiratory_rate", xlab = "respiratory_rate")
hist(wide$SpO2, main = "Distribution of SpO2", xlab = "SpO2")
#calculate the precentage of missing values in baseline data
sum(is.na(lab))/nrow(lab)
lab<-read.csv("lab and vitals.csv")
baseline<-read.csv("baselines.csv",stringsAsFactors=TRUE)
library(tidyverse)
library(skimr)
library(knitr)
library(kableExtra)
par(mfcol = c(1,3))
hist(baseline$Age, main = "Distribution of age", xlab = "age")
hist(baseline$bmi, main = "Distribution of bmi", xlab = "bmi")
hist(baseline$duration_sym, main = "Distribution of duration of symptoms", xlab = "duration of symptoms")
lab%>%
group_by(subject,name)%>%
summarise(ave=mean(value,na.rm=TRUE))%>%
rename(value=ave)%>%
ungroup()->vital
wide<-pivot_wider(data=vital,names_from = name,values_from = value,values_fill = NA) %>% rename(diastolic_blood_pressure="s_bp_noninvasive (d)",
systolic_blood_pressure="vs_bp_noninvasive (s)",
heart_rate="vs_hr_hr",respiratory_rate="xp_resp_rate_pt",SpO2="xp_resp_spo2")
par(mfcol = c(2,3))
hist(wide$diastolic_blood_pressure, main = "Distribution of diastolic blood pressure", xlab = "diastolic_blood_pressure")
hist(wide$systolic_blood_pressure, main = "Distribution of systolic_blood_pressure", xlab = "systolic_blood_pressure")
hist(wide$heart_rate, main = "Distribution of heart_rate", xlab = "heart_rate")
hist(wide$respiratory_rate, main = "Distribution of respiratory_rate", xlab = "respiratory_rate")
hist(wide$SpO2, main = "Distribution of SpO2", xlab = "SpO2")
library(e1071)
data.frame(var=c("diastolic_blood_pressure","systolic_blood_pressure","heart_rate","respiratory_rate","SpO2"),kurtosis=c(kurtosis(wide$diastolic_blood_pressure),kurtosis(wide$systolic_blood_pressure),kurtosis(wide$heart_rate) ,kurtosis(wide$respiratory_rate),kurtosis(wide$SpO2)),skewness=c(skewness(wide$diastolic_blood_pressure),skewness(wide$systolic_blood_pressure),skewness(wide$heart_rate) ,skewness(wide$respiratory_rate),skewness(wide$SpO2)))%>% kable() %>% kable_styling()
alldata<-merge(wide,baseline,by.x="subject",by.y="mrn",all.y=TRUE)%>%
dplyr::select(-1)
set.seed(960515)
train_ind<-sample(nrow(alldata),nrow(alldata)/2)
train<-alldata[train_ind,]
test<-alldata[-train_ind,]
library(class)
KNN.error <- function(Train, Test, k=1) {
Train.x <- scale(Train[,which(names(Train) !="event")])
Test.x <- scale(Test[,which(names(Test) !="event")])
pred.class=knn(Train.x, Test.x, Train$event, k=k)
mean(Test$event != pred.class)
}
set.seed(960515)
train_num<-train
test_num<-test
indx <- sapply(train_num, is.factor)
train_num[indx] <- lapply(train_num[indx], function(x) as.numeric(x)-1)
indx <- sapply(test_num, is.factor)
test_num[indx] <- lapply(test_num[indx], function(x) as.numeric(x)-1)
#perform 5-fold cross-validation
error<-NULL
best_k=1
cv_error=1
folds <- cut(seq(1,nrow(train)),breaks=5,labels=FALSE)
for (k in 1:20){
for(m in 1:5){
testIndexes <- which(folds == m, arr.ind=TRUE)
cv_tr <- train_num[-testIndexes, ]
cv_tst <- train_num[testIndexes, ]
error[m]<-KNN.error(cv_tr,cv_tst,k)
}
if(cv_error>mean(error))
{
best_k<-k
cv_error<-mean(error)
}
}
KNN_error<-KNN.error(train_num,test_num,k=best_k)
