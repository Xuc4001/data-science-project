---
title: "Project Report"
author: "Xu Chen"
date: "8/4/2020"
output:
  html_document:
    code_folding: show
    theme: readable
    toc: yes
    toc_depth: 5
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '5'
editor_options:
  chunk_output_type: inline
---
## Reasearch Objective
Some patients diagnosed with COVID suffer from acute respiratory distress and have acute difficulties in breathing, and may require artificial respiratory support through an invasive mechanical ventilator, a process known as intubation. The goal of the report is to build a predictive model that will predict whether a COVID+ patient needs intubation. 

## Method

The codes used in this part are shown in Appendix.

### Feature Engineering
```{r,include=FALSE}
lab<-read.csv("lab and vitals.csv")
baseline<-read.csv("baselines.csv",stringsAsFactors=TRUE)
```
```{r,include=FALSE}
library(tidyverse)
library(skimr)
library(knitr)
library(kableExtra)
```

#### Baseline Data

Baseline dataset contains no missing value, and all predictors may be correlated with acute respiratory distress syndrome. I decide to keep all 26 features. 

The distribution of age is slightly left skewed. The distributions of bmi and duration of symptoms are right skewed. Others are categorical variables
```{r,echo=FALSE}
par(mfcol = c(1,3))
hist(baseline$Age, main = "Distribution of age", xlab = "age")
hist(baseline$bmi, main = "Distribution of bmi", xlab = "bmi")
hist(baseline$duration_sym, main = "Distribution of duration of symptoms", xlab = "duration of symptoms")
```



#### Labs and Vitals Data

The data contains 5 types of vital signs consisting of diastolic blood pressure, systolic blood pressure, heart rate, respiratory rate, and pulse oxygen saturation (SpO2). 

The vital signs are measured multiple times for each patient during the course of hospitalization, and the precentage of the missing value is 12%. The measured times also varies with each individual. 

Due to those properties of the dataset, I decide to use the average value of each type of vital signs as feature. 

According to the skewness,the distributions of the diastolic blood pressure, systolic blood pressure, heart rate, and pulse oxygen saturation (SpO2) are slightly left skewed; The distribution of respiratory rate is slghtly right skewed. 

According to the kurtosis, the shapes of all variables are flatter than normal distribution. No missing value is in those five features.


```{r,include=FALSE}
lab%>%
  group_by(subject,name)%>%
  summarise(ave=mean(value,na.rm=TRUE))%>%
  rename(value=ave)%>%
  ungroup()->vital
wide<-pivot_wider(data=vital,names_from = name,values_from = value,values_fill = NA) %>% rename(diastolic_blood_pressure="s_bp_noninvasive (d)",
         systolic_blood_pressure="vs_bp_noninvasive (s)",
         heart_rate="vs_hr_hr",respiratory_rate="xp_resp_rate_pt",SpO2="xp_resp_spo2")
```

```{r,echo=FALSE}
par(mfcol = c(2,3))
hist(wide$diastolic_blood_pressure, main = "Distribution of diastolic blood pressure", xlab = "diastolic_blood_pressure")
hist(wide$systolic_blood_pressure, main = "Distribution of systolic_blood_pressure", xlab = "systolic_blood_pressure")
hist(wide$heart_rate, main = "Distribution of heart_rate", xlab = "heart_rate")
hist(wide$respiratory_rate, main = "Distribution of respiratory_rate", xlab = "respiratory_rate")
hist(wide$SpO2, main = "Distribution of SpO2", xlab = "SpO2")
```

```{r,echo=FALSE}
library(e1071)
data.frame(var=c("diastolic_blood_pressure","systolic_blood_pressure","heart_rate","respiratory_rate","SpO2"),kurtosis=c(kurtosis(wide$diastolic_blood_pressure),kurtosis(wide$systolic_blood_pressure),kurtosis(wide$heart_rate) ,kurtosis(wide$respiratory_rate),kurtosis(wide$SpO2)),skewness=c(skewness(wide$diastolic_blood_pressure),skewness(wide$systolic_blood_pressure),skewness(wide$heart_rate) ,skewness(wide$respiratory_rate),skewness(wide$SpO2)))%>% kable() %>% kable_styling()
```

#### Analytical Sample
```{r,include=FALSE}
alldata<-merge(wide,baseline,by.x="subject",by.y="mrn",all.y=TRUE)%>%
  dplyr::select(-1)
```
I merge the baseline data with the processed vitals data by matching the variable "mrn" in the baseline data with the variable "subject" in processed vitals data. The two variables both represent the unique patient medical record number. 

The final analytical sample contains 31 predictors and 1345 observations without missing value.

### Predictive Modeling



#### Data Split
```{r,include = FALSE}
set.seed(960515)
train_ind<-sample(nrow(alldata),nrow(alldata)/2)
train<-alldata[train_ind,]
test<-alldata[-train_ind,]
```
The chosen way to estimate prediction error in this report is to calculate the missclassification rate on a test dataset (i.e. generalization error). 

Therefore, I split the data randomly into training (50%,n=672) and test set (50%,n=673). I use the training data to train the model in different ways and use the test data to calculate test error to find which model performs best.

#### Classification Methods

##### k-Nearest Neighbor(KNN)

```{r,include = FALSE}
library(class)
KNN.error <- function(Train, Test, k=1) {
  Train.x <- scale(Train[,which(names(Train) !="event")])
  Test.x <- scale(Test[,which(names(Test) !="event")])
  pred.class=knn(Train.x, Test.x, Train$event, k=k)
  mean(Test$event != pred.class)
}

```

```{r,include = FALSE}
set.seed(960515)
train_num<-train
test_num<-test

indx <- sapply(train_num, is.factor)
train_num[indx] <- lapply(train_num[indx], function(x) as.numeric(x)-1)
indx <- sapply(test_num, is.factor)
test_num[indx] <- lapply(test_num[indx], function(x) as.numeric(x)-1)
#perform 5-fold cross-validation
error<-NULL
best_k=1
cv_error=1
folds <- cut(seq(1,nrow(train)),breaks=5,labels=FALSE)
for (k in 1:20){
  for(m in 1:5){
  testIndexes <- which(folds == m, arr.ind=TRUE)
  cv_tr <- train_num[-testIndexes, ]
  cv_tst <- train_num[testIndexes, ]
  error[m]<-KNN.error(cv_tr,cv_tst,k)
  }
  if(cv_error>mean(error))
     {
       best_k<-k
       cv_error<-mean(error)
     }
}
KNN_error<-KNN.error(train_num,test_num,k=best_k)
```
k-Nearest Neighbor is a distance based algorithm, which is affected by the scale of the variables. I use 5-fold cross-validation to select the K value (ranged from 1 to 20), which has the smallest CV-error. The chosen K is 18. 
Then, I use the chosen K to calculate the test error. Every training data and test data (including the training data and the test data in different cv-folds) are scaled separately to avoid that the training data "see" the information from the test data.


##### Logistic, LDA, QDA
```{r,include = FALSE}
logistic.error <- function(Train, Test) {
  logistic.out <- glm(event ~., data=Train, family="binomial")
  pred.class <- predict(logistic.out, Test, type="response")
  pred.class <- ifelse(pred.class >0.5, "Yes", "No")
  mean(Test$event != pred.class)
}

logistic_error<-logistic.error(train,test)
logistic_error
```

```{r,include = FALSE}
library(MASS)
LDA.error <- function(Train, Test) {
  lda.out <- lda(event ~ ., data=Train)
  pred.class = predict(lda.out, Test)$class
  mean(Test$event != pred.class)
}
LDA_error<-LDA.error(train,test)
LDA_error
```

```{r,include = FALSE}
QDA.error <- function(Train, Test) {
  qda.out <- qda(event ~ ., data=Train)
  pred.class = predict(qda.out, Test)$class
  mean(Test$event != pred.class)
}
QDA_error<-QDA.error(train,test)
QDA_error
```

Logistic, LDA and QDA are all good classification methods. I use the training data to train the model in three ways and use the test data to calculate the test errors.

#### Subset Selection Methods

Best Subset Selection can not work in this problem for computational reasons (p=31). 

I apply Forward Stepwise Selection and Backward Stepwise Selection methods to the logistic regression model, and select the single best model using Akaike information criterion.

```{r,include = FALSE}
library(bestglm)
backward.obj = glm(event ~ ., family="binomial", data=train)
bestForward<-stepAIC(backward.obj,direction="backward",trace=FALSE)

pred.class <- predict(bestForward, test, type="response")

pred.class <- ifelse(pred.class >0.50, "Yes", "No")

BackStp_error <- mean(test$event != pred.class)
BackStp_error
```



```{r,include = FALSE}
forward.obj = glm(event ~ 1, family="binomial", data=train)
bestForward<-stepAIC(forward.obj,direction="forward",scope = list(lower = formula(forward.obj), upper = formula(backward.obj)),trace=FALSE)

pred.class <- predict(bestForward, test, type="response")

pred.class <- ifelse(pred.class >0.50, "Yes", "No")

ForStp_error <- mean(test$event != pred.class)
ForStp_error
```
#### Shrinkage Methods

Lasso, Ridge regression, and Elastic Net (alpha=0.5) methods are used. 5-fold cross-validation is performed to choose the best lambda(ranged from 10^-2 to 10^10) in three methods, because selecting a good value of labmda for fit is critical. 

In lasso method, the chose lambda is 0.0231 In Ridge regression method, the chose lambda is 0.0305 In Elastic Net (alpha=0.5) method, the chose lambda is 0.0534 
```{r,include = FALSE}
library(glmnet)
x=model.matrix(event~.,train)[,-1]
y=train$event
newx=model.matrix(event~.,test)[,-1]
newy=test$event

grid =10^seq(10,-2,length =100) ##define range of lambda values
```


````{r,include = FALSE}
##Run cross validation to choose lambda 
set.seed(960515)
cv1=cv.glmnet(x,y, family="binomial",lambda=grid,nfolds=5,alpha=1)
cv1.final = glmnet(x,y, family="binomial", lambda = cv1$lambda.1se,alpha=1)
pred.class <- predict(cv1.final, newx, type="response")
pred.class <- ifelse(pred.class >0.50,"Yes", "No")
Elast_alpha1_error <- mean(newy != pred.class)
Elast_alpha1_error
```

````{r,include = FALSE}
##Run cross validation to choose lambda
set.seed(960515)
cv.5=cv.glmnet(x,y, family="binomial",lambda=grid,nfolds=5,alpha=0.5)
cv.5.final = glmnet(x,y, family="binomial", lambda = cv.5$lambda.1se,alpha=.5)
pred.class <- predict(cv.5.final, newx, type="response")
pred.class <- ifelse(pred.class >0.50,"Yes", "No")
Elast_alpha.5_error <- mean(newy != pred.class)
Elast_alpha.5_error
```

````{r,include = FALSE}
##Run cross validation to choose lambda 
set.seed(960515)
cv0=cv.glmnet(x,y, family="binomial",lambda=grid,nfolds=5,alpha=0)
cv0.final = glmnet(x,y, family="binomial", lambda = cv0$lambda.1se,alpha=)
pred.class <- predict(cv0.final, newx, type="response")
pred.class <- ifelse(pred.class >0.50,"Yes", "No")
Elast_alpha0_error <- mean(newy != pred.class)
Elast_alpha0_error
```

#### TREE-Based Method
##### A single tree
I grow a single tree using gini index. Since a single tree might cause overfitting problem, we should prune the tree. The tuning parameter alpha controls a trade-off between the subtree’s complexity and its fit to the training data. 

I perform 10-fold cross-validation to select the optimal alpha. The chosen alpha is 5.


```{r,include = FALSE}
library(tree)
set.seed(960515)
mytree<-tree(event~.,data=train,method = "gini")
mytree.cv<-cv.tree(mytree,FUN=prune.misclass,K=10)
final.tree=prune.tree(mytree,best=mytree.cv$size[mytree.cv$dev==min(mytree.cv$dev)])
mypredict<-predict(final.tree,newdata=test,type="class")
tmp=table(mypredict,test$event)
tree_error<-1-sum(diag(tmp)/sum(tmp))
tree_error
```



##### Bagged Tree and Random Forest

I use bagging and random forest methods.
In bagged tree, I generate 1000 different bootstrapped training data sets, and in each training data sets, all 31 preditors are considered at each split.


In random forest, I generate 1000 different bootstrapped training data sets, and in each training data sets, a random selection of 6 preditors are considered at each split.

##### Boosting methods

I use 4 boosting models: gradient boosted model with depth one, gradient boosted model with depth two, adaboost model with depth one and adaboost model with depth two. 

Boosting can overfit if the number of trees is too large. The shrinkage parameter labmda can controls the rate at which boosting learn. I use 5-fold cross-validation to chose the optimal labmda (selection range: [0,0.1]). The number of trees is 2000. 

The chosen labmda using gradient boosted model with depth one is 0.01. The chosen labmda using gradient boosted model with depth two is 0.008. The chosen labmda using adaboost model with depth one is 0.006. The chosen labmda using adaboost model with depth one is 0.01. 


```{r,include = FALSE}
library(randomForest)
set.seed(960515)
bagged.tree<-randomForest(event~., data = train, mtry = ncol(train)-1, ntree = 1000,importance=TRUE)
mypredict.bagg = predict(bagged.tree, test, type = "response")
tmp.bagg.tree <- table(mypredict.bagg, test$event)
bagg_error <- 1 - sum(diag(tmp.bagg.tree)/sum(tmp.bagg.tree))
bagg_error
```

```{r,include = FALSE}
set.seed(960515)
rf<-randomForest(event~., data = train, mtry = 6, ntree = 1000,importance=TRUE)
mypredict.rf = predict(rf, test, type = "response")
tmp.rf.tree <- table(mypredict.rf, test$event)
rf_error <- 1 - sum(diag(tmp.rf.tree)/sum(tmp.rf.tree))
rf_error
```



```{r,include = FALSE}
train_gbm<-train
test_gbm<-test
train_gbm$event.int<-as.integer(train_gbm$event)-1
test_gbm$event.int<-as.integer(test_gbm$event)-1
```

```{r,include = FALSE}
library(caret)
library(gbm)
set.seed(960515)
caretGrid.depth.1 <- expand.grid(interaction.depth = 1, 
                                 n.trees = 2000, 
                                 shrinkage = seq(0, 0.01, by = 0.001), 
                                 n.minobsinnode = 10)
gbm.d1 <- caret::train(event ~.-event.int , distribution = "bernoulli", data = train_gbm, 
                       method = "gbm", trControl = trainControl(method="cv", number=5),
                       verbose = F, tuneGrid = caretGrid.depth.1)
gbm_d1 <- gbm(event.int ~ . -event , data = train_gbm, n.trees = 2000, interaction.depth = 1, shrinkage = gbm.d1$bestTune$shrinkage,  distribution = 'bernoulli')

```
```{r,include = FALSE}
gbm.d1_error=mean((predict(gbm_d1, newdata = test_gbm, n.trees = 2000, type = 'response') >0.5) != test_gbm$event.int)
gbm.d1_error
```

```{r,include = FALSE}
set.seed(960515)
caretGrid.depth.2 <- expand.grid(interaction.depth = 2, 
                                 n.trees = 2000, 
                                 shrinkage = seq(0, 0.01, by = 0.001), 
                                 n.minobsinnode = 10)
gbm.d2 <- caret::train(event ~.-event.int , distribution = "bernoulli", data = train_gbm, 
                       method = "gbm", trControl = trainControl(method="cv", number=5),
                       verbose = F, tuneGrid = caretGrid.depth.2)
gbm_d2 <- gbm(event.int ~ . -event , data = train_gbm, n.trees = 2000, interaction.depth = 2, shrinkage = gbm.d2$bestTune$shrinkage,  distribution = 'bernoulli')

```
```{r,include = FALSE}
gbm.d2_error=mean((predict(gbm_d2, newdata = test_gbm, n.trees = 2000, type = 'response') >0.5) != test_gbm$event.int)
gbm.d2_error
```

```{r,include = FALSE}
set.seed(960515)

ada.d1 <- caret::train(event ~.-event.int , distribution = "adaboost", data = train_gbm,method = "gbm", trControl = trainControl(method="cv", number=5), verbose = F, tuneGrid = caretGrid.depth.1)
ada_d1 <- gbm(event.int ~ . -event , data = train_gbm, n.trees = 2000, interaction.depth = 1, shrinkage = ada.d1$bestTune$shrinkage,  distribution = 'adaboost')

```
```{r,include = FALSE}

ada.d1_error=mean((predict(ada_d1, newdata = test_gbm, n.trees = 2000, type = 'response') >0.5) != test_gbm$event.int)
ada.d1_error
```

```{r,include = FALSE}
set.seed(960515)
ada.d2 <- caret::train(event ~.-event.int , distribution = "adaboost", data = train_gbm, method = "gbm", trControl = trainControl(method="cv", number=5),verbose = F, tuneGrid = caretGrid.depth.2)
ada_d2 <- gbm(event.int ~ . -event , data = train_gbm, n.trees = 2000, interaction.depth = 2, shrinkage = ada.d2$bestTune$shrinkage,  distribution = 'adaboost')

```
```{r,include = FALSE}
ada.d2_error=mean((predict(ada_d2, newdata = test_gbm, n.trees = 2000, type = 'response') >0.5) != test_gbm$event.int)
ada.d2_error
```


#### Support Vector Machines

The predictors should be scaled since distance of points are considered to construct decision boundaries. The used function "svm" scales predictors internally by default.


##### Support Vector Classifier

In support vector classifier, the cost parameter is a budget for the amount
that the margin can be violated. I use 10-fold cross-validation to select the optimal cost parameter (range=c(0.001 , 0.01, 0.1, 1,5,10,100)).  The chosen cost parameter is 0.01.
```{r,include = FALSE}
library(e1071)
set.seed(960515)
tune<-tune.svm(event~.,data=train,kernel="linear",cost=c(0.001 , 0.01, 0.1, 1,5,10,100))

```


```{r,include = FALSE}
svm.obj1<-svm(event~.-event,data=train,kernel="linear",cost=tune$best.parameters$cost)
svclass_error<-mean(predict(svm.obj1,newdata=test)!=test$event)
```

##### Support Vector Machines with “Radial” Kernel 

In SVM using a non-linear kernel, both gamma and cost need to be chosen via a cross-validation. I use 10-fold cross-validation to select the optimal cost parameter (range=c(0.001 , 0.01, 0.1, 1,5,10,100)) and gamma (range=c(0.1,0.01,0.001)). The chosen cost parameter is 5. The chosen gamma is 0.001.
```{r,include = FALSE}
set.seed(960515)
tune<-tune.svm(event~.,data=train,kernel="radial",cost=c(0.001 , 0.01, 0.1, 1,5,10,100),gamma=c(0.1,0.01,0.001))
tune$best.parameters
```


```{r,include = FALSE}
svm.obj2<-svm(event~.-event,data=train,kernel="radial",cost=tune$best.parameters$cost,gamma=tune$best.parameters$gamma)
radial_error<-mean(predict(svm.obj2,newdata=test)!=test$event)
```

#### Neural Network

In neural network, since the scaling of the inputs determines the effective scaling of the weights in the bottom layer, predictors are scaled to have mean zero and standard deviation one in both training and test dataset. 
```{r,include = FALSE}
train_num[,which(names(alldata) !="event")]=scale(train_num[,which(names(alldata) !="event")])
test_num[,which(names(alldata) !="event")]=scale(test_num[,which(names(alldata) !="event")])
```
```{r,include = FALSE}
train_num$event<-factor(train_num$event)
test_num$event<-factor(test_num$event)
```

Also, I perform 5-fold cross-validation to select the optimal weight decay (range=c(0,0.001,0.01,0.1,1,5,10,20)) to aviod overfitting data at the global minimum. The chosen decay is 10.

```{r,include = FALSE}
library(nnet)
set.seed(960515)
nnetGrid<-expand.grid(.size=25,.decay=c(0,0.001,0.01,0.1,1,5,10,20))
nnetfit<-caret::train(event~.,data=train_num,method="nnet",trControl=trainControl(method="cv",number=5),tuneGrid=nnetGrid,trace=F)
```

```{r,include = FALSE}
neu.obj<-nnet(event~.,data=train_num,size=25,decay=10,trace=FALSE)
nn_error<-mean(ifelse(predict(neu.obj,newdata = test_num)>0.5,1,0)!=test_num$event)
```


## Result

The test errors using all methods described in  Predictive Modeling part are shown in the table below.
```{r,echo=FALSE}
library(knitr)
library(kableExtra)
data.frame(Type=c("Classification Methods","","","","Stepwise Selection Methods","","Shrinkage Methods","","","Tree-Based Methods","","","","","","","Support Vector Machines","","Neural Network"),
                  Method=c("KNN","Logistic Regression","LDA","QDA", "Backward Stepwise Selection", "Forward Stepwise Selection", "LASSO", "Elastic Net (alpha=0.5)", "Ridege Regression", "Single Tree","Bagged Tree","Random Forest","Gradient Boosted Model (d=1)","Gradient Boosted Model (d=2)","Adaboost Model (d=1)", "Adaboost Model (d=2)","Support Vector Classifier", "Support Vector Machines with “Radial” Kernel","Neural Network" ), Test_Error=round(c(KNN_error, logistic_error, LDA_error, QDA_error, BackStp_error,ForStp_error,Elast_alpha1_error,Elast_alpha.5_error,Elast_alpha0_error,tree_error,bagg_error,rf_error,gbm.d1_error,gbm.d2_error,ada.d1_error,ada.d2_error,svclass_error,radial_error,nn_error), digits=3)) %>% kable() %>% kable_styling()
```

## Conclusion

Among all methods, many methods' test errors are near 0.2, indicating that Logistic Regression, LDA, two Stepwise Selection methods, three shrinkage methods, four boosting methods, two Support Vector Machines methods , and Neural Network have a relatively better performance in this dataset. KNN, QDA, the single tree, Bagged tree, random forest do not fit very well in this data.


Since the lowest test error is 0.178 using LDA method, I would suggest using Linear Discriminant Analysis method and all 31 preditors.


Below is the standardized coefficients of the discriminant function of predictors, indicating the relative importance of the independent variables in predicting the dependent. Heart_rate (-0.7132), diastolic_blood_pressure (-0.4428), smoke_vapeYes (0.5546) have the relative large value. 


```{r}
lda.obj<-lda(event ~ ., data=train)
lda.obj$scaling
```
Therefore, heart rate, diastolic blood pressure, and ever smoker/vaper are very important factors for prediction.


Also, it should be noted that LDA assumes that the observations are drawn from a Gaussian distribution with a common covariance matrix in each class.
If assumptions hold, LDA does well when classes are well separated. But if the distribution of predictors is very skewed or the number of predictors is very large, LDA usually perfroms very poor. 


## Appendix
```{r}
library(tidyverse)
library(skimr)
```
#### loda data
```{r}

lab<-read.csv("lab and vitals.csv")
baseline<-read.csv("baselines.csv",stringsAsFactors=TRUE)
```
#### Baseline Data
```{r}
# calculate the precentage of missing values in baseline data
sum(is.na(baseline))/nrow(baseline)
```

```{r,eval=FALSE}
##plot the histogram of age,bmi and duration.
par(mfcol = c(1,3))
hist(baseline$Age, main = "Distribution of age", xlab = "age")
hist(baseline$bmi, main = "Distribution of bmi", xlab = "bmi")
hist(baseline$duration_sym, main = "Distribution of duration of symptoms", xlab = "duration of symptoms")
```
```{r,eval=FALSE}
skim(baseline)
```

#### Labs and Vitals Data

```{r,eval=FALSE}
#calculate the precentage of missing values in baseline data
sum(is.na(lab))/nrow(lab)
```

```{r}
## calculate the mean value, group by patient and lab name.
lab%>%
  group_by(subject,name)%>%
  summarise(ave=mean(value,na.rm=TRUE))%>%
  rename(value=ave)%>%
  ungroup()->vital
## Pivot data from long to wide
wide<-pivot_wider(data=vital,names_from = name,values_from = value,values_fill = NA) %>% rename(diastolic_blood_pressure="s_bp_noninvasive (d)",
         systolic_blood_pressure="vs_bp_noninvasive (s)",
         heart_rate="vs_hr_hr",respiratory_rate="xp_resp_rate_pt",SpO2="xp_resp_spo2")
```

```{r,eval=FALSE}
##plot the histogram of the processed features
par(mfcol = c(2,3))
hist(wide$diastolic_blood_pressure, main = "Distribution of diastolic blood pressure", xlab = "diastolic_blood_pressure")
hist(wide$systolic_blood_pressure, main = "Distribution of systolic_blood_pressure", xlab = "systolic_blood_pressure")
hist(wide$heart_rate, main = "Distribution of heart_rate", xlab = "heart_rate")
hist(wide$respiratory_rate, main = "Distribution of respiratory_rate", xlab = "respiratory_rate")
hist(wide$SpO2, main = "Distribution of SpO2", xlab = "SpO2")
```


```{r,eval=FALSE}
library(e1071)
library(knitr)
library(kableExtra)
## calculate the kurtosis and skewness of processed features.
data.frame(var=c("diastolic_blood_pressure","systolic_blood_pressure","heart_rate","respiratory_rate","SpO2"),kurtosis=c(kurtosis(wide$diastolic_blood_pressure),kurtosis(wide$systolic_blood_pressure),kurtosis(wide$heart_rate) ,kurtosis(wide$respiratory_rate),kurtosis(wide$SpO2)),skewness=c(skewness(wide$diastolic_blood_pressure),skewness(wide$systolic_blood_pressure),skewness(wide$heart_rate) ,skewness(wide$respiratory_rate),skewness(wide$SpO2)))%>% kable() %>% kable_styling()
```

#### Analytical Sample
```{r}
## merge two datasets
alldata<-merge(wide,baseline,by.x="subject",by.y="mrn",all.y=TRUE)%>%
  dplyr::select(-1)
```

#### Data Split
```{r}
##split the data randomly into training (50%,n=672) and test set(50%,n=673)
set.seed(960515)
train_ind<-sample(nrow(alldata),nrow(alldata)/2)
train<-alldata[train_ind,]
test<-alldata[-train_ind,]
```


#### KNN


```{r}
##knn error function
library(class)
KNN.error <- function(Train, Test, k=1) {
  Train.x <- scale(Train[,which(names(Train) !="event")])
  Test.x <- scale(Test[,which(names(Test) !="event")])
  pred.class=knn(Train.x, Test.x, Train$event, k=k)
  mean(Test$event != pred.class)
}

```


```{r}
set.seed(960515)
## covert factor variables to numeric variables to allow scaling.
train_num<-train
test_num<-test
indx <- sapply(train_num, is.factor)
train_num[indx] <- lapply(train_num[indx], function(x) as.numeric(x)-1)
indx <- sapply(test_num, is.factor)
test_num[indx] <- lapply(test_num[indx], function(x) as.numeric(x)-1)
#perform 5-fold cross-validation to choose the best K
error<-NULL
best_k=1
cv_error=1
folds <- cut(seq(1,nrow(train)),breaks=5,labels=FALSE)
for (k in 1:20){
  for(m in 1:5){
  testIndexes <- which(folds == m, arr.ind=TRUE)
  cv_tr <- train_num[-testIndexes, ]
  cv_tst <- train_num[testIndexes, ]
  error[m]<-KNN.error(cv_tr,cv_tst,k)
  }
  if(cv_error>mean(error))
     {
       best_k<-k
       cv_error<-mean(error)
     }
}
```

```{r}
##calculate test error
KNN_error<-KNN.error(train_num,test_num,k=best_k)
```


#### logistic
```{r}
logistic.error <- function(Train, Test) {
  logistic.out <- glm(event ~., data=Train, family="binomial")
  pred.class <- predict(logistic.out, Test, type="response")
  pred.class <- ifelse(pred.class >0.5, "Yes", "No")
  mean(Test$event != pred.class)
}
##calculate test error
logistic_error<-logistic.error(train,test)
```

#### LDA
```{r}
library(MASS)
LDA.error <- function(Train, Test) {
  lda.out <- lda(event ~ ., data=Train)
  pred.class = predict(lda.out, Test)$class
  mean(Test$event != pred.class)
}
##calculate test error
LDA_error<-LDA.error(train,test)
```

#### QDA
```{r}
QDA.error <- function(Train, Test) {
  qda.out <- qda(event ~ ., data=Train)
  pred.class = predict(qda.out, Test)$class
  mean(Test$event != pred.class)
}
##calculate test error
QDA_error<-QDA.error(train,test)
```


#### Backward Stepwise Selection

```{r}
library(bestglm)
##fit logistic regression with all predictors.
backward.obj = glm(event ~ ., family="binomial", data=train)
##perform backward stepwise selection using AIC 
bestForward<-stepAIC(backward.obj,direction="backward",trace=FALSE)
##calculate test error
pred.class <- predict(bestForward, test, type="response")
pred.class <- ifelse(pred.class >0.50, "Yes", "No")
BackStp_error <- mean(test$event != pred.class)
```
#### Forward Stepwise Selection

```{r}
##fit logistic regression with null predictor.
forward.obj = glm(event ~ 1, family="binomial", data=train)
##perform forward stepwise selection using AIC 
bestForward<-stepAIC(forward.obj,direction="forward",scope = list(lower = formula(forward.obj), upper = formula(backward.obj)),trace=FALSE)

##calculate test error
pred.class <- predict(bestForward, test, type="response")
pred.class <- ifelse(pred.class >0.50, "Yes", "No")
ForStp_error <- mean(test$event != pred.class)
```
#### LASSO

```{r}
library(glmnet)
x=model.matrix(event~.,train)[,-1]
y=train$event
newx=model.matrix(event~.,test)[,-1]
newy=test$event
grid =10^seq(10,-2,length =100) ##define range of lambda values
```




````{r}
##Run cross validation to choose lambda 
set.seed(960515)
cv1=cv.glmnet(x,y, family="binomial",lambda=grid,nfolds=5,alpha=1)
```

```{r}
cv1.final = glmnet(x,y, family="binomial", lambda = cv1$lambda.1se,alpha=1)
pred.class <- predict(cv1.final, newx, type="response")
pred.class <- ifelse(pred.class >0.50,"Yes", "No")
##calculate test error
Elast_alpha1_error <- mean(newy != pred.class)
```

#### Elastic Net (alpha=0.5)
````{r}
##Run cross validation to choose lambda
set.seed(960515)
cv.5=cv.glmnet(x,y, family="binomial",lambda=grid,nfolds=5,alpha=0.5)
```
```{r}
cv.5.final = glmnet(x,y, family="binomial", lambda = cv.5$lambda.1se,alpha=.5)
pred.class <- predict(cv.5.final, newx, type="response")
pred.class <- ifelse(pred.class >0.50,"Yes", "No")
##calculate test error
Elast_alpha.5_error <- mean(newy != pred.class)

```

#### Ridge Regression
````{r}
##Run cross validation to choose lambda 
set.seed(960515)
cv0=cv.glmnet(x,y, family="binomial",lambda=grid,nfolds=5,alpha=0)
```
```{r}
cv0.final = glmnet(x,y, family="binomial", lambda = cv0$lambda.1se,alpha=)
##calculate test error
pred.class <- predict(cv0.final, newx, type="response")
pred.class <- ifelse(pred.class >0.50,"Yes", "No")
Elast_alpha0_error <- mean(newy != pred.class)
```

#### A Single Tree
```{r}
library(tree)
set.seed(960515)
# fit a single tree
mytree<-tree(event~.,data=train,method = "gini")
##Run cross validation to choose alpha
mytree.cv<-cv.tree(mytree,FUN=prune.misclass,K=10)
mytree.cv$size[mytree.cv$dev==min(mytree.cv$dev)]
```


```{r}
final.tree=prune.tree(mytree,best=mytree.cv$size[mytree.cv$dev==min(mytree.cv$dev)])
##calculate test error
mypredict<-predict(final.tree,newdata=test,type="class")
tmp=table(mypredict,test$event)
tree_error<-1-sum(diag(tmp)/sum(tmp))
```

#### Bagged Tree
```{r}
library(randomForest)
set.seed(960515)
bagged.tree<-randomForest(event~., data = train, mtry = ncol(train)-1, ntree = 1000,importance=TRUE)
##calculate test error
mypredict.bagg = predict(bagged.tree, test, type = "response")
tmp.bagg.tree <- table(mypredict.bagg, test$event)
bagg_error <- 1 - sum(diag(tmp.bagg.tree)/sum(tmp.bagg.tree))
```

#### Random Forest
```{r}
set.seed(960515)
rf<-randomForest(event~., data = train, mtry = 6, ntree = 1000,importance=TRUE)
##calculate test error
mypredict.rf = predict(rf, test, type = "response")
tmp.rf.tree <- table(mypredict.rf, test$event)
rf_error <- 1 - sum(diag(tmp.rf.tree)/sum(tmp.rf.tree))
```

#### Gradient Boosted Model with depth one 

```{r}
train_gbm<-train
test_gbm<-test
train_gbm$event.int<-as.integer(train_gbm$event)-1
test_gbm$event.int<-as.integer(test_gbm$event)-1
```

```{r}
library(caret)
library(gbm)
set.seed(960515)
##Run cross validation to choose shrinkage
caretGrid.depth.1 <- expand.grid(interaction.depth = 1, 
                                 n.trees = 2000, 
                                 shrinkage = seq(0, 0.01, by = 0.001), 
                                 n.minobsinnode = 10)
gbm.d1 <- caret::train(event ~.-event.int , distribution = "bernoulli", data = train_gbm, method = "gbm", trControl = trainControl(method="cv", number=5),verbose = F, tuneGrid = caretGrid.depth.1)
```

```{r}
gbm_d1 <- gbm(event.int ~ . -event , data = train_gbm, n.trees = 2000, interaction.depth = 1, shrinkage = gbm.d1$bestTune$shrinkage,  distribution = 'bernoulli')
##calculate test error
gbm.d1_error=mean((predict(gbm_d1, newdata = test_gbm, n.trees = 2000, type = 'response') >0.5) != test_gbm$event.int)
```

```{r}
set.seed(960515)
##Run cross validation to choose shrinkage
caretGrid.depth.2 <- expand.grid(interaction.depth = 2, 
                                 n.trees = 2000, 
                                 shrinkage = seq(0, 0.01, by = 0.001), 
                                 n.minobsinnode = 10)
gbm.d2 <- caret::train(event ~.-event.int , distribution = "bernoulli", data = train_gbm, method = "gbm", trControl = trainControl(method="cv", number=5),verbose = F, tuneGrid = caretGrid.depth.2)
```
#### Gradient Boosted Model with depth two
```{r}
gbm_d2 <- gbm(event.int ~ . -event , data = train_gbm, n.trees = 2000, interaction.depth = 2, shrinkage = gbm.d2$bestTune$shrinkage,  distribution = 'bernoulli')

##calculate test error
gbm.d2_error=mean((predict(gbm_d2, newdata = test_gbm, n.trees = 2000, type = 'response') >0.5) != test_gbm$event.int)
```

#### Adaboost model with depth one 
```{r}
set.seed(960515)
##Run cross validation to choose shrinkage
ada.d1 <- caret::train(event ~.-event.int , distribution = "adaboost", data = train_gbm,method = "gbm", trControl = trainControl(method="cv", number=5), verbose = F, tuneGrid = caretGrid.depth.1)

```

```{r}
ada_d1 <- gbm(event.int ~ . -event , data = train_gbm, n.trees = 2000, interaction.depth = 1, shrinkage = ada.d1$bestTune$shrinkage,  distribution = 'adaboost')
##calculate test error
ada.d1_error=mean((predict(ada_d1, newdata = test_gbm, n.trees = 2000, type = 'response') >0.5) != test_gbm$event.int)
```
#### Adaboost model with depth two 
```{r}
set.seed(960515)
##Run cross validation to choose shrinkage
ada.d2 <- caret::train(event ~.-event.int , distribution = "adaboost", data = train_gbm, method = "gbm", trControl = trainControl(method="cv", number=5),verbose = F, tuneGrid = caretGrid.depth.2)
```

```{r}
ada_d2 <- gbm(event.int ~ . -event , data = train_gbm, n.trees = 2000, interaction.depth = 2, shrinkage = ada.d2$bestTune$shrinkage,  distribution = 'adaboost')
##calculate test error
ada.d2_error=mean((predict(ada_d2, newdata = test_gbm, n.trees = 2000, type = 'response') >0.5) != test_gbm$event.int)
```

#### linear classifier

```{r}
library(e1071)
set.seed(960515)
## Run cross validation to choose the cost
tune<-tune.svm(event~.,data=train,kernel="linear",cost=c(0.001 , 0.01, 0.1, 1,5,10,100))
```

```{r}
svm.obj1<-svm(event~.-event,data=train,kernel="linear",cost=tune$best.parameters$cost)
##calculate test error
svclass_error<-mean(predict(svm.obj1,newdata=test)!=test$event)
```


#### “radial” kernel 

```{r}
set.seed(960515)
## Run cross validation to choose the cost
tune<-tune.svm(event~.,data=train,kernel="radial",cost=c(0.001 , 0.01, 0.1, 1,5,10,100),gamma=c(0.1,0.01,0.001))
```

```{r}
svm.obj2<-svm(event~.-event,data=train,kernel="radial",cost=tune$best.parameters$cost,gamma=tune$best.parameters$gamma)
##calculate test error
radial_error<-mean(predict(svm.obj2,newdata=test)!=test$event)
```

#### Neural Network

```{r}
train_num[,which(names(alldata) !="event")]=scale(train_num[,which(names(alldata) !="event")])
test_num[,which(names(alldata) !="event")]=scale(test_num[,which(names(alldata) !="event")])
```
```{r}
train_num$event<-factor(train_num$event)
test_num$event<-factor(test_num$event)
```


```{r}
library(nnet)
set.seed(960515)
###Run cross validation to choose the decay
nnetGrid<-expand.grid(.size=25,.decay=c(0,0.001,0.01,0.1,1,5,10,20))
nnetfit<-caret::train(event~.,data=train_num,method="nnet",trControl=trainControl(method="cv",number=5),tuneGrid=nnetGrid,trace=F)
```
```{r}
neu.obj<-nnet(event~.,data=train_num,size=25,decay=10,trace=FALSE)
##calculate test error
nn_error<-mean(ifelse(predict(neu.obj,newdata = test_num)>0.5,1,0)!=test_num$event)
```

#### Result Table
```{r,eval=FALSE}
library(knitr)
library(kableExtra)
data.frame(Type=c("Classification Methods","","","","Stepwise Selection Methods","","Shrinkage Methods","","","Tree-Based Methods","","","","","","","Support Vector Machines","","Neural Network"),
                  Method=c("KNN","Logistic Regression","LDA","QDA", "Backward Stepwise Selection", "Forward Stepwise Selection", "LASSO", "Elastic Net (alpha=0.5)", "Ridege Regression", "Single Tree","Bagged Tree","Random Forest","Gradient Boosted Model (d=1)","Gradient Boosted Model (d=2)","Adaboost Model (d=1)", "Adaboost Model (d=2)","Support Vector Classifier", "Support Vector Machines with “Radial” Kernel","Neural Network" ), Test_Error=round(c(KNN_error, logistic_error, LDA_error, QDA_error, BackStp_error,ForStp_error,Elast_alpha1_error,Elast_alpha.5_error,Elast_alpha0_error,tree_error,bagg_error,rf_error,gbm.d1_error,gbm.d2_error,ada.d1_error,ada.d2_error,svclass_error,radial_error,nn_error), digits=3)) %>% kable() %>% kable_styling()
```

